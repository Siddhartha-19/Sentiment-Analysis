{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hzriYj0MakKP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA5Vlo4m5jj3"
      },
      "outputs": [],
      "source": [
        "fp=open(\"/content/drive/MyDrive/Colab Notebooks/positive.review\", 'r')\n",
        "fn=open(\"/content/drive/MyDrive/Colab Notebooks/negative.review\", 'r')\n",
        "filesdata=[]\n",
        "pcount=0\n",
        "ncount=0\n",
        "sentiments=[]\n",
        "def getdata(line):\n",
        "  data=[]\n",
        "  words=line.split(\" \")[:-1]\n",
        "  for word in words:\n",
        "    w=word.split(\":\")[0]\n",
        "    count=int(word.split(\":\")[1])\n",
        "    while count>0:\n",
        "      data.append(w)\n",
        "      count-=1\n",
        "  return \" \".join(data)\n",
        "for line in fp.readlines():\n",
        "  pcount+=1\n",
        "  filesdata.append(getdata(line))\n",
        "  sentiments.append(0)\n",
        "for line in fn.readlines():\n",
        "  ncount+=1\n",
        "  filesdata.append(getdata(line))\n",
        "  sentiments.append(1)\n",
        "random.seed(0)\n",
        "temp=list(zip(filesdata, sentiments))\n",
        "random.shuffle(temp)\n",
        "filesdata, sentiments=zip(*temp)\n",
        "filesdata, sentiments=list(filesdata),list(sentiments)\n",
        "dict_for_words={}\n",
        "fp=open(\"/content/drive/MyDrive/Colab Notebooks/positive.review\", 'r')\n",
        "fn=open(\"/content/drive/MyDrive/Colab Notebooks/negative.review\", 'r')\n",
        "for line in fp.readlines():\n",
        "  tokens=line.split(\" \")\n",
        "  for word in tokens[:-1]:\n",
        "    name, counts = word.split(\":\")\n",
        "    if name not in dict_for_words:\n",
        "        dict_for_words[name] = 0\n",
        "    dict_for_words[name] += int(counts)\n",
        "for line in fn.readlines():\n",
        "  tokens=line.split(\" \")\n",
        "  for word in tokens[:-1]:\n",
        "    name, counts = word.split(\":\")\n",
        "    if name not in dict_for_words:\n",
        "        dict_for_words[name] = 0\n",
        "    dict_for_words[name] += int(counts)\n",
        "print(len(dict_for_words))\n",
        "vectorizer = CountVectorizer()\n",
        "filesdata[:1]\n",
        "bow_matrix=vectorizer.fit_transform(filesdata)\n",
        "bow_matrix[:1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow_matrix, sentiments, test_size=0.2)\n",
        "c_train=Counter(y_train)\n",
        "c_test=Counter(y_test)\n",
        "print(\"class pos train\",c_train[0])\n",
        "print(\"class neg train\",c_train[1])\n",
        "print(\"class pos test\",c_test[0])\n",
        "print(\"class neg test\",c_test[1])\n",
        "print(X_train.shape)\n",
        "def MLP(sizes,activation,num_epochs):\n",
        "   classifier=MLPClassifier(hidden_layer_sizes=sizes, activation=activation, solver='adam', learning_rate='constant', learning_rate_init=0.001, max_iter=num_epochs)\n",
        "   classifier.fit(X_train,y_train)\n",
        "   pred=classifier.predict(X_test)\n",
        "   print(len(sizes),\" layers  \",activation)\n",
        "   print(classification_report(y_test,pred))\n",
        "   print()\n",
        "MLP([100],\"relu\",100)\n",
        "MLP([100,100],\"relu\",100)\n",
        "MLP([100,100,100],\"relu\",100)\n",
        "MLP([100],\"tanh\",100)\n",
        "MLP([100,100],\"tanh\",100)\n",
        "MLP([100,100,100],\"tanh\",100)\n",
        "MLP([100],\"logistic\",100)\n",
        "MLP([100,100],\"logistic\",100)\n",
        "MLP([100,100,100],\"logistic\",100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "NNrn0w-VQX84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = dict()\n",
        "def add_to_dict(d, filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      line = line.split(' ')\n",
        "\n",
        "      try:\n",
        "        d[line[0]] = np.array(line[1:], dtype=float)\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "add_to_dict(words, '/content/glove.6B.100d.txt')"
      ],
      "metadata": {
        "id": "Y0B_PB1HhLux"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def message_to_word_vectors(message, word_dict=words):\n",
        "  processed_list_of_tokens = message.split()\n",
        "\n",
        "  vectors = []\n",
        "\n",
        "  for token in processed_list_of_tokens:\n",
        "    if token not in word_dict:\n",
        "      continue\n",
        "    \n",
        "    token_vector = word_dict[token]\n",
        "    vectors.append(token_vector)\n",
        "  \n",
        "  return np.array(vectors, dtype=float)\n",
        "all_word_vector_sequences = []\n",
        "\n",
        "for d in filesdata :\n",
        "  message_as_vector_seq = message_to_word_vectors(d)\n",
        "    \n",
        "  if message_as_vector_seq.shape[0] == 0:\n",
        "    message_as_vector_seq = np.zeros(shape=(1, 100))\n",
        "  all_word_vector_sequences.append(message_as_vector_seq)\n",
        "sequence_lengths = []\n",
        "for i in range(len(all_word_vector_sequences)):\n",
        "  sequence_lengths.append(len(all_word_vector_sequences[i]))\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.hist(sequence_lengths)\n",
        "# print(pd.Series(sequence_lengths).describe())\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "def pad_X(X, desired_sequence_length=2073):\n",
        "  X_copy = deepcopy(X)\n",
        "\n",
        "  for i, x in enumerate(X):\n",
        "    x_seq_len = x.shape[0]\n",
        "    sequence_length_difference = desired_sequence_length - x_seq_len\n",
        "    \n",
        "    pad = np.zeros(shape=(sequence_length_difference,100))\n",
        "\n",
        "    X_copy[i] = np.concatenate([x, pad])\n",
        "  \n",
        "  return np.array(X_copy).astype(float)\n",
        "\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(all_word_vector_sequences, sentiments, test_size=0.2)\n",
        "X_train_new = pad_X(X_train_new)\n",
        "x,y,z=X_train_new.shape\n",
        "X_train_new=X_train_new.reshape((x,y*z))\n",
        "X_test_new = pad_X(X_test_new)\n",
        "x,y,z=X_test_new.shape\n",
        "X_test_new=X_test_new.reshape((x,y*z))\n",
        "classifier=MLPClassifier(hidden_layer_sizes=(100,100), activation=\"relu\", solver='adam', learning_rate='constant', learning_rate_init=0.001, max_iter=100)\n",
        "classifier.fit(X_train_new,y_train_new)\n",
        "pred=classifier.predict(X_test_new)\n",
        "print(classification_report(y_test_new,pred))"
      ],
      "metadata": {
        "id": "DDVaeX4D7eZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}